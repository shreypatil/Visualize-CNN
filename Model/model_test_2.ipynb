{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3988c9c4-fbf8-4848-bfcb-c54387196dbc",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc6df1e-b1e4-4cc5-8fb1-55a77afbf01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef88e9-8613-4ac1-a946-605905bdcf95",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5736e937-25cd-4f1b-9058-ff919b0bef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81f040b-9e75-40a0-bb69-b524e108a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e2616b-2686-41d3-a35b-2ed6ba44ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1000, batch_size=1, pre_train=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flag = True\n",
    "        \n",
    "        # self.vgg_name=vgg_name\n",
    "        self.pre_train = pre_train\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.switches = []\n",
    "        self.org_shapes = []\n",
    "        self.unpool_shapes = []\n",
    "        self.feature_maps = []\n",
    "        \n",
    "        self.conv = nn.ModuleList()\n",
    "        self.pool = nn.ModuleList()\n",
    "        self.norm = nn.ModuleList()\n",
    "        \n",
    "        self.linear = nn.ModuleList()\n",
    "        self.drop = nn.ModuleList()\n",
    "        \n",
    "        # =======================================================\n",
    "        \n",
    "        # layers = cfg[vgg_name]\n",
    "        layers = cfg['VGG11']\n",
    "        \n",
    "        curr_channels = self.in_channels\n",
    "        for i in range(len(layers)):\n",
    "            if layers[i] == 'M':\n",
    "                continue\n",
    "            \n",
    "            self.conv.append(nn.Conv2d(in_channels=curr_channels, out_channels=layers[i], kernel_size=3, stride=1, padding=1))\n",
    "            \n",
    "            if i+1 < len(layers) and layers[i+1] == 'M':\n",
    "                self.pool.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True))\n",
    "            else:\n",
    "                self.pool.append(nn.Identity())\n",
    "            \n",
    "            # self.norm.append(nn.BatchNorm2d(num_features=layers[i]))\n",
    "            self.norm.append(nn.Identity())\n",
    "            curr_channels = layers[i]\n",
    "        \n",
    "        \n",
    "        ## Average Pool\n",
    "        self.pool.append(nn.AvgPool2d(kernel_size=7, stride=1))\n",
    "        \n",
    "        ## Linear 1\n",
    "        # self.linear.append(nn.Linear(in_features=25088, out_features=4096)) # If average pool not used\n",
    "        self.linear.append(nn.Linear(in_features=512, out_features=4096))\n",
    "        self.drop.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        ## Linear 2\n",
    "        self.linear.append(nn.Linear(in_features=4096, out_features=4096))\n",
    "        self.drop.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        ## Linear 3: Output\n",
    "        self.linear.append(nn.Linear(in_features=4096, out_features=self.out_channels))\n",
    "        self.drop.append(nn.Identity())\n",
    " \n",
    "        # =======================================================\n",
    "\n",
    "        ## Initialize weights\n",
    "        if pre_train:\n",
    "            self._init_weights_1()\n",
    "        else:\n",
    "            self._init_weights_2()\n",
    "            \n",
    "    \n",
    "    def _init_weights_1(self):\n",
    "        \n",
    "        # pre_model = vgg11(weights='DEFAULT')\n",
    "        pre_model = vgg11()\n",
    "        \n",
    "        for i in range(len(self.conv)):\n",
    "            with torch.no_grad():\n",
    "                if i < 2:\n",
    "                    self.conv[i].weight.copy_(pre_model.features[i * 3].weight)\n",
    "                    \n",
    "                    if pre_model.features[i * 3].bias is not None:\n",
    "                        self.conv[i].bias.copy_(pre_model.features[i * 3].bias)\n",
    "                else:\n",
    "                    self.conv[i].weight.copy_(pre_model.features[(i + 4) + (i - 2) + ((i - 1) // 3)].weight)\n",
    "                    \n",
    "                    if pre_model.features[(i + 4) + (i - 2) + ((i - 1) // 3)].bias is not None:\n",
    "                        self.conv[i].weight.copy_(pre_model.features[(i + 4) + (i - 2) + ((i - 1) // 3)].bias)\n",
    "                    \n",
    "        for i in range(len(self.Linear)):\n",
    "            with torch.no_grad():\n",
    "                    self.linear[i].weight.copy_(pre_model.classifier[i * 3].weight)\n",
    "                    \n",
    "                    if pre_model.classifier[i * 3].bias is not None:\n",
    "                        self.linear[i].weight.copy_(pre_model.classifier[i * 3].bias)\n",
    "    \n",
    "    def _init_weights_2(self):\n",
    "        \n",
    "        for M in self.modules():\n",
    "            if isinstance(M, nn.Conv2d):\n",
    "                # nn.init.kaiming_uniform_(M.weight)\n",
    "                nn.init.constant_(M.weight, 1e-2)\n",
    "\n",
    "                if M.bias is not None:\n",
    "                    nn.init.constant_(M.bias, 0)\n",
    "\n",
    "#             elif isinstance(M, nn.BatchNorm2d):\n",
    "#                 # BatchNorm with a mean of 0 = bias and a variance of 1e-2 = weight:\n",
    "#                 nn.init.constant_(M.weight, 1e-2)\n",
    "                \n",
    "#                 if M.bias is not None:\n",
    "#                     nn.init.constant_(M.bias, 0)\n",
    "\n",
    "            elif isinstance(M, nn.Linear):\n",
    "                # nn.init.kaiming_uniform_(M.weight)\n",
    "                nn.init.constant_(M.weight, 1e-2)\n",
    "                \n",
    "                if M.bias is not None:\n",
    "                    nn.init.constant_(M.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(len(self.conv)):\n",
    "           \n",
    "            if self.flag:\n",
    "                self.org_shapes.append(x.shape)\n",
    "            \n",
    "            x = self.conv[i](x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "            if isinstance(self.pool[i], nn.MaxPool2d):\n",
    "                if self.flag:\n",
    "                    self.unpool_shapes.append(x.shape)\n",
    "                    \n",
    "                x, indices = self.pool[i](x)\n",
    "                \n",
    "                if self.flag:\n",
    "                    self.switches.append(indices)\n",
    "                \n",
    "            else:\n",
    "                if self.flag:\n",
    "                    self.unpool_shapes.append(None)\n",
    "                    \n",
    "                x = self.pool[i](x)\n",
    "                \n",
    "                if self.flag:\n",
    "                    self.switches.append(None)\n",
    "            \n",
    "            ## Local Contrast Normalization across feature maps (similar to AlexNet)\n",
    "            x = self.norm[i](x)\n",
    "            \n",
    "            if self.flag:\n",
    "                self.feature_maps.append(x)\n",
    "        \n",
    "        \n",
    "        ## Average Pool\n",
    "        x = self.pool[-1](x)\n",
    "        \n",
    "        ## Flatten tensor for Linear Layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        for i in range(len(self.linear) - 1):\n",
    "            x = self.linear[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.drop[i](x)\n",
    "            \n",
    "        x = self.linear[-1](x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        self.flag = False\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31dc0e95-16f8-4e10-9218-66167bac7103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([5, 3, 224, 224])\n",
      "y.shape: torch.Size([5, 2])\n",
      "\n",
      "y: tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def convnet_test():\n",
    "    x = torch.randn((5, 3, 224, 224))\n",
    "    model = ConvNet(in_channels = 3, out_channels = 2)\n",
    "    \n",
    "    y = model(x)\n",
    "    print('x.shape:', x.shape)\n",
    "    print('y.shape:', y.shape)\n",
    "    print()\n",
    "    \n",
    "    print(f'y: {y}')\n",
    "    \n",
    "convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6d861-6fbb-4d41-ad62-35000a4029bb",
   "metadata": {},
   "source": [
    "## DeConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9ff4d6-7eab-4644-b176-b8128b14a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeConvNet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_model = model\n",
    "        \n",
    "        self.deconv = nn.ModuleList()\n",
    "        self.unpool = nn.ModuleList()\n",
    "        \n",
    "        # =======================================================\n",
    "        \n",
    "        # layers = cfg[vgg_name]\n",
    "        layers = cfg['VGG11']\n",
    "        \n",
    "        curr_channels = model.in_channels\n",
    "        \n",
    "        for i in range(len(layers)):\n",
    "            if layers[i] == 'M':\n",
    "                continue\n",
    "            \n",
    "            self.deconv.append(nn.ConvTranspose2d(in_channels=layers[i], out_channels=curr_channels, kernel_size=3, stride=1, padding=1, output_padding=0))\n",
    "            \n",
    "            if i+1 < len(layers) and layers[i+1] == 'M':\n",
    "                self.unpool.append(nn.MaxUnpool2d(kernel_size=2, stride=2, padding=0))\n",
    "            else:\n",
    "                self.unpool.append(nn.Identity())\n",
    "            \n",
    "            curr_channels = layers[i]\n",
    "        \n",
    "        # ======================================================================\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for i in range(len(self.deconv)):\n",
    "            with torch.no_grad():\n",
    "                self.deconv[i].weight.copy_(self.conv_model.conv[i].weight)\n",
    "                # self.deconv[i].weight = self.conv_model.conv[i].weight\n",
    "                # self.deconv[i].weight = nn.Parameter(self.conv_model.conv[i].weight.detach().clone())\n",
    "                # self.deconv[i].weight = nn.Parameter(torch.empty_like(self.conv_model.conv[i].weight).copy_(self.conv_model.conv[i].weight))\n",
    "                \n",
    "                # if self.conv_model.conv[i].bias is not None:\n",
    "                #     self.deconv[i].bias.copy_(self.conv_model.conv[i].bias)\n",
    "                \n",
    "    \n",
    "    def forward(self, layer_no=1):\n",
    "            \n",
    "        idx = layer_no - 1\n",
    "        x = self.conv_model.feature_maps[idx].detach().clone()\n",
    "        \n",
    "        ## Set all Activation except target activation position as 0\n",
    "        temp = x[..., 0, 0].clone()\n",
    "        x.fill_(0)\n",
    "        x[..., 0, 0] = temp\n",
    "        \n",
    "        for i in range(idx, -1, -1):\n",
    "            \n",
    "            if self.conv_model.switches[i] is not None:\n",
    "                x = self.unpool[i](x, self.conv_model.switches[i], output_size=self.conv_model.unpool_shapes[i])\n",
    "                \n",
    "            x = F.relu(x)\n",
    "            \n",
    "            x = self.deconv[i](x)\n",
    "            \n",
    "            ## Just to make sure that this code runs for other image shapes (i.e., other than 224 x 224)\n",
    "            ## BUT its better to change the model paddings accordingly without using this\n",
    "            if x.shape != self.conv_model.org_shapes[i]:\n",
    "                # skipping batch_size and no._of_channels\n",
    "                x = TF.resize(x, size=self.conv_model.org_shapes[i][2:])\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b181e6-43e9-4d72-b489-6265dcf125b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([5, 3, 224, 224])\n",
      "y1.shape: torch.Size([5, 2])\n",
      "\n",
      "y1: tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]], grad_fn=<SoftmaxBackward0>)\n",
      "y2.shape: torch.Size([5, 3, 224, 224])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def deconvnet_test():\n",
    "    x = torch.randn((5, 3, 224, 224))\n",
    "    \n",
    "    print('x.shape:', x.shape)\n",
    "    \n",
    "    conv = ConvNet(in_channels = 3, out_channels = 2)\n",
    "    \n",
    "    y1 = conv(x)\n",
    "    \n",
    "    print('y1.shape:', y1.shape)\n",
    "    print()\n",
    "    \n",
    "    print(f'y1: {y1}')\n",
    "    \n",
    "    \n",
    "    deconv = DeConvNet(conv)\n",
    "    y2 = deconv(layer_no = 5)\n",
    "    \n",
    "    print('y2.shape:', y2.shape)\n",
    "    print()\n",
    "    \n",
    "    # print(f'y2:{y2}')\n",
    "    # print()\n",
    "\n",
    "deconvnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd8222-bd64-4633-99c6-b88a9109d44b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- ImageNet: resize smallest dimenstion to $256$, crop center region as $256 \\times 256$\n",
    "- Subtracting the per-pixel mean\n",
    "- Then using $10$ different sub-crops of size $224 \\times 224$ (corners + center with(out) horizontal flips)\n",
    "- Multiple different crops and flips of each training sample to boost training set size\n",
    "\n",
    "**Stochastic Gradient Descent**\n",
    "- Mini-batch size = $128$\n",
    "- Learning Rate = $10^{-2}$\n",
    "- Momentum = $0.9$\n",
    "- Epochs = $70$\n",
    "\n",
    "Visualization of $1_{st}$ layer during training reveals that a few of them dominate (Fig. 6(a)). To combat this:\n",
    "- Normalize each filter in the covolution layer whose $RMS$ value exceeds a fixed radius of $10^{-1}$ to this fixed radius\n",
    "- This is curcial, especially in the $1_{st}$ layer of the model, where the input images are roughly in the range $[-128, 128]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3460db6f-1c4d-4655-828b-8452669710ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vinet]",
   "language": "python",
   "name": "conda-env-vinet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
