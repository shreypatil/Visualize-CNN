{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3988c9c4-fbf8-4848-bfcb-c54387196dbc",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc6df1e-b1e4-4cc5-8fb1-55a77afbf01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef88e9-8613-4ac1-a946-605905bdcf95",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea6cb84-1817-4d0d-9650-84fff83fdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=2, batch_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flag = True\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.switches = []\n",
    "        self.org_shapes = []\n",
    "        self.unpool_shapes = []\n",
    "        self.feature_maps = []\n",
    "        \n",
    "        self.conv = nn.ModuleList()\n",
    "        self.pool = nn.ModuleList()\n",
    "        self.norm = nn.ModuleList()\n",
    "        \n",
    "        self.linear = nn.ModuleList()\n",
    "        self.drop = nn.ModuleList()\n",
    "        \n",
    "        ## Layer 1\n",
    "        self.conv.append(nn.Conv2d(in_channels=self.in_channels, out_channels=96, kernel_size=7, stride=2, padding=1))\n",
    "        self.pool.append(nn.MaxPool2d(kernel_size=3, stride=2, padding = 1, return_indices=True))\n",
    "        # self.norm.append(nn.BatchNorm2d(num_features=96))\n",
    "        self.norm.append(nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2.0))\n",
    "        \n",
    "        ## Layer 2\n",
    "        self.conv.append(nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=0))\n",
    "        self.pool.append(nn.MaxPool2d(kernel_size=3, stride=2, padding = 1, return_indices=True))\n",
    "        # self.norm.append(nn.BatchNorm2d(num_features=256))\n",
    "        self.norm.append(nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2.0))\n",
    "        \n",
    "        ## Layer 3\n",
    "        self.conv.append(nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1))\n",
    "        self.pool.append(nn.Identity())\n",
    "        self.norm.append(nn.Identity())\n",
    "\n",
    "        ## Layer 4\n",
    "        self.conv.append(nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1))\n",
    "        self.pool.append(nn.Identity())\n",
    "        self.norm.append(nn.Identity())\n",
    "        \n",
    "        ## Layer 5\n",
    "        self.conv.append(nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1))\n",
    "        self.pool.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=0, return_indices=True))\n",
    "        self.norm.append(nn.Identity())\n",
    "        \n",
    "        ## Layer 6\n",
    "        self.linear.append(nn.Linear(9216, 4096))\n",
    "        self.drop.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        ## Layer 7\n",
    "        self.linear.append(nn.Linear(4096, 4096))\n",
    "        self.drop.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        ## Output\n",
    "        self.linear.append(nn.Linear(4096, self.out_channels))\n",
    "        self.drop.append(nn.Identity())\n",
    "        \n",
    "        # ## Initialize weights\n",
    "        # self.apply(self._init_weights_1)\n",
    "        self._init_weights_2()\n",
    "    \n",
    "#     def _init_weights_1(self, module):\n",
    "#         if isinstance(module, nn.Conv2d):\n",
    "#             module.bias.data.zero_()\n",
    "#             module.weight.data.fill_(1e-2)\n",
    "\n",
    "#         if isinstance(module, nn.BatchNorm2d):\n",
    "#             # BatchNorm with a mean of 0 = bias and a variance of 1 = weight:\n",
    "#             module.bias.data.zero_()\n",
    "#             module.weight.data.fill_(1e-2)\n",
    "\n",
    "#         elif isinstance(module, nn.Linear):\n",
    "#             module.bias.data.zero_()\n",
    "#             module.weight.data.fill_(1e-2)\n",
    "    \n",
    "    def _init_weights_2(self):\n",
    "        \n",
    "        for M in self.modules():\n",
    "            if isinstance(M, nn.Conv2d):\n",
    "                # nn.init.kaiming_uniform_(M.weight)\n",
    "                nn.init.constant_(M.weight, 1e-2)\n",
    "\n",
    "                if M.bias is not None:\n",
    "                    nn.init.constant_(M.bias, 0)\n",
    "\n",
    "            elif isinstance(M, nn.BatchNorm2d):\n",
    "                # BatchNorm with a mean of 0 = bias and a variance of 1e-2 = weight:\n",
    "                nn.init.constant_(M.weight, 1e-2)\n",
    "                \n",
    "                if M.bias is not None:\n",
    "                    nn.init.constant_(M.bias, 0)\n",
    "\n",
    "            elif isinstance(M, nn.Linear):\n",
    "                # nn.init.kaiming_uniform_(M.weight)\n",
    "                nn.init.constant_(M.weight, 1e-2)\n",
    "                \n",
    "                if M.bias is not None:\n",
    "                    nn.init.constant_(M.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(len(self.conv)):\n",
    "           \n",
    "            if self.flag:\n",
    "                self.org_shapes.append(x.shape)\n",
    "            \n",
    "            x = self.conv[i](x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "            if isinstance(self.pool[i], nn.MaxPool2d):\n",
    "                if self.flag:\n",
    "                    self.unpool_shapes.append(x.shape)\n",
    "                    \n",
    "                x, indices = self.pool[i](x)\n",
    "                \n",
    "                if self.flag:\n",
    "                    self.switches.append(indices)\n",
    "                \n",
    "            else:\n",
    "                if self.flag:\n",
    "                    self.unpool_shapes.append(None)\n",
    "                    \n",
    "                x = self.pool[i](x)\n",
    "                \n",
    "                if self.flag:\n",
    "                    self.switches.append(None)\n",
    "                \n",
    "            ## Local Contrast Normalization across feature maps (similar to AlexNet)\n",
    "            x = self.norm[i](x)\n",
    "            \n",
    "            if self.flag:\n",
    "                self.feature_maps.append(x)\n",
    "                \n",
    "            \n",
    "        ## Flatten tensor for Linear Layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        for i in range(len(self.linear) - 1):\n",
    "            \n",
    "            x = self.linear[i](x)\n",
    "            x = self.drop[i](x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = self.linear[-1](x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        self.flag = False\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dc0e95-16f8-4e10-9218-66167bac7103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([5, 3, 224, 224])\n",
      "y.shape: torch.Size([5, 2])\n",
      "\n",
      "y: tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def convnet_test():\n",
    "    x = torch.randn((5, 3, 224, 224))\n",
    "    model = ConvNet(in_channels = 3, out_channels = 2)\n",
    "    \n",
    "    y = model(x)\n",
    "    print('x.shape:', x.shape)\n",
    "    print('y.shape:', y.shape)\n",
    "    print()\n",
    "    \n",
    "    print(f'y: {y}')\n",
    "    \n",
    "convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6d861-6fbb-4d41-ad62-35000a4029bb",
   "metadata": {},
   "source": [
    "## DeConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e9ff4d6-7eab-4644-b176-b8128b14a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeConvNet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_model = model\n",
    "        \n",
    "        self.deconv = nn.ModuleList()\n",
    "        self.unpool = nn.ModuleList()\n",
    "        \n",
    "        ## Layer 1\n",
    "        # Check if we need to set `out_padding` needs to be set\n",
    "        self.deconv.append(nn.ConvTranspose2d(in_channels=96, out_channels=self.conv_model.in_channels, kernel_size=7, stride=2, padding=1, output_padding=1))\n",
    "        self.unpool.append(nn.MaxUnpool2d(kernel_size=3, stride=2, padding = 0))\n",
    "        \n",
    "        ## Layer 2\n",
    "        # Check if we need to set `out_padding` needs to be set\n",
    "        self.deconv.append(nn.ConvTranspose2d(in_channels=256, out_channels=96, kernel_size=5, stride=2, padding=0, output_padding=0))\n",
    "        self.unpool.append(nn.MaxUnpool2d(kernel_size=3, stride=2, padding = 0))\n",
    "        \n",
    "        ## Layer 3\n",
    "        # Check if we need to set `out_padding` needs to be set\n",
    "        self.deconv.append(nn.ConvTranspose2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1, output_padding=0))\n",
    "        self.unpool.append(nn.Identity())\n",
    "\n",
    "        ## Layer 4\n",
    "        # Check if we need to set `out_padding` needs to be set\n",
    "        self.deconv.append(nn.ConvTranspose2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1, output_padding=0))\n",
    "        self.unpool.append(nn.Identity())\n",
    "        \n",
    "        ## Layer 5\n",
    "        # Check if we need to set `out_padding` needs to be set\n",
    "        self.deconv.append(nn.ConvTranspose2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1, output_padding=0))\n",
    "        self.unpool.append(nn.MaxUnpool2d(kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self._init_weights_2()\n",
    "        \n",
    "    def _init_weights_2(self):\n",
    "        for i in range(len(self.deconv)):\n",
    "            with torch.no_grad():\n",
    "                self.deconv[i].weight.copy_(self.conv_model.conv[i].weight)\n",
    "                # self.deconv[i].weight = self.conv_model.conv[i].weight\n",
    "                # self.deconv[i].weight = nn.Parameter(self.conv_model.conv[i].weight.detach().clone())\n",
    "                # self.deconv[i].weight = nn.Parameter(torch.empty_like(self.conv_model.conv[i].weight).copy_(self.conv_model.conv[i].weight))\n",
    "                \n",
    "    \n",
    "    def forward(self, layer_no=1):\n",
    "            \n",
    "        idx = layer_no - 1\n",
    "        x = self.conv_model.feature_maps[idx].detach().clone()\n",
    "        \n",
    "        ## Set all Activation except target activation position as 0\n",
    "        temp = x[..., 0, 0].clone()\n",
    "        x.fill_(0)\n",
    "        x[..., 0, 0] = temp\n",
    "        \n",
    "        for i in range(idx, -1, -1):\n",
    "            \n",
    "            if self.conv_model.switches[i] is not None:\n",
    "                x = self.unpool[i](x, self.conv_model.switches[i], output_size=self.conv_model.unpool_shapes[i])\n",
    "                \n",
    "            x = F.relu(x)\n",
    "            \n",
    "            x = self.deconv[i](x)\n",
    "            \n",
    "            ## Just to make sure that this code runs for other image shapes (i.e., other than 224 x 224)\n",
    "            ## BUT its better to change the model paddings accordingly without using this\n",
    "            if x.shape != self.conv_model.org_shapes[i]:\n",
    "                # skipping batch_size and no._of_channels\n",
    "                x = TF.resize(x, size=self.conv_model.org_shapes[i][2:])\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b181e6-43e9-4d72-b489-6265dcf125b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([5, 3, 224, 224])\n",
      "y1.shape: torch.Size([5, 2])\n",
      "\n",
      "y1: tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]], grad_fn=<SoftmaxBackward0>)\n",
      "y2.shape: torch.Size([5, 3, 224, 224])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def deconvnet_test():\n",
    "    x = torch.randn((5, 3, 224, 224))\n",
    "    \n",
    "    print('x.shape:', x.shape)\n",
    "    \n",
    "    conv = ConvNet(in_channels = 3, out_channels = 2)\n",
    "    \n",
    "    y1 = conv(x)\n",
    "    \n",
    "    print('y1.shape:', y1.shape)\n",
    "    print()\n",
    "    \n",
    "    print(f'y1: {y1}')\n",
    "    \n",
    "    \n",
    "    deconv = DeConvNet(conv)\n",
    "    y2 = deconv(layer_no = 5)\n",
    "    \n",
    "    print('y2.shape:', y2.shape)\n",
    "    print()\n",
    "    \n",
    "    # print(f'y2:{y2}')\n",
    "    # print()\n",
    "\n",
    "deconvnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd8222-bd64-4633-99c6-b88a9109d44b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- ImageNet: resize smallest dimenstion to $256$, crop center region as $256 \\times 256$\n",
    "- Subtracting the per-pixel mean\n",
    "- Then using $10$ different sub-crops of size $224 \\times 224$ (corners + center with(out) horizontal flips)\n",
    "- Multiple different crops and flips of each training sample to boost training set size\n",
    "\n",
    "**Stochastic Gradient Descent**\n",
    "- Mini-batch size = $128$\n",
    "- Learning Rate = $10^{-2}$\n",
    "- Momentum = $0.9$\n",
    "- Epochs = $70$\n",
    "\n",
    "Visualization of $1_{st}$ layer during training reveals that a few of them dominate (Fig. 6(a)). To combat this:\n",
    "- Normalize each filter in the covolution layer whose $RMS$ value exceeds a fixed radius of $10^{-1}$ to this fixed radius\n",
    "- This is curcial, especially in the $1_{st}$ layer of the model, where the input images are roughly in the range $[-128, 128]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3460db6f-1c4d-4655-828b-8452669710ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vinet]",
   "language": "python",
   "name": "conda-env-vinet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
